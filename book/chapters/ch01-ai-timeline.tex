\chapter{The Long Road to Here: AI's Timeline from 1940 to 2026}

This chapter covers ancient history. Not in a boring way. In the way that matters for understanding why 2023 felt different.

If you want to know why your job is changing, you need to know why the previous 80 years of AI didn't change it. The pattern is clear: AI repeatedly hit walls. Walls of computing power. Walls of data. Walls of fundamental understanding. What changed in 2022-2023 wasn't magic. It was the convergence of three things finally becoming true at the same time.

\section{The Dream Begins (1940s-1950s)}

The idea that machines could think isn't new. What's new is that we built machines that could prove it.

In 1950, Alan Turing asked a question that would haunt computer science for decades: \emph{Can machines think?} He didn't pretend to answer it philosophically. Instead, he proposed something practical: if a machine could hold a conversation indistinguishable from a human, then for practical purposes, it thinks. The Turing Test. Simple. Dangerous. Still relevant.

By the mid-1950s, researchers had built the first neural networks. Warren McCulloch and Walter Pitts showed in 1943 that networks of simple artificial neurons could perform logical operations. Marvin Minsky and Dean Edmonds built the SNARC in 1951—the first neural net machine. It worked. It was slow. It was expensive. It would be forgotten for decades.

Then came the Dartmouth Workshop in the summer of 1956. This is where AI got its name and its mission. John McCarthy, Marvin Minsky, Claude Shannon, and others gathered to test a bold assertion: that every aspect of human intelligence could be precisely described and simulated by a machine.

They believed it. They really believed it. Within a decade, one of them predicted machines would match human intelligence. The timeline? Ten years. Twenty years max. Humanity would have artificial general intelligence by the late 1960s.

This is the optimism before the wall.

\section{Early Wins, Hidden Limits (1956-1974)}

The 1960s were impressive. Computers solved algebra word problems. They proved geometry theorems. They beat humans at checkers. ELIZA, a chatbot that barely did anything but restate what you said, fooled people into thinking it was a therapist. The "Look Ma, no hands!" era, as John McCarthy called it. Astonishment at anything clever.

Government funding poured in. Millions of dollars from ARPA (now DARPA) and other agencies. Entire laboratories dedicated to cracking human intelligence. MIT, Stanford, Carnegie Mellon, Edinburgh. The smartest people in the world were working on this problem.

But there was a problem with the solution. Three of them, actually.

First: computers were weak. Not slow. Weak. A model trained on 20 words of vocabulary was impressive because computers couldn't hold more. Memory was measured in kilobytes. Processing speed in kilohertz. When Hans Moravec calculated what it would take to match human vision, he realized you'd need a computer millions of times faster than anything that existed.

Second: the combinatorial explosion. Most intelligent tasks require searching through possibilities—millions, billions, trillions of them. The early approaches tried to search every option. For non-trivial problems, this is impossible. You'd need to search until the heat death of the universe.

Third: common sense. A five-year-old knows that birds fly but penguins don't. That when you knock over a glass of water, it spills. That people have intentions and beliefs. Encoding this into a machine seemed to require billions of atomic facts, manually entered. No one in 1970 could build that database. No one knew how machines might learn it.

The researchers didn't give up. They kept working. But by 1974, the promises had clearly exceeded reality. The U.S. and British governments cut funding. The Lighthill Report in the UK concluded that AI had failed to achieve its "grandiose objectives." The money stopped. Over 300 AI companies shut down or went bankrupt.

This was the First AI Winter. It lasted six years.

\section{Revival and Optimism Trap (1980-1987)}

By 1980, expert systems arrived. Not general intelligence. Something narrower. Systems that encoded expert knowledge in a specific domain—medical diagnosis, equipment configuration, insurance underwriting. They worked. More importantly, they made money.

R1, an expert system for Digital Equipment Corporation, saved the company \$40 million a year by 1986. Suddenly corporations around the world were building expert systems. By 1985, companies were spending over a billion dollars on AI, mostly on in-house projects.

The industry boomed. Specialized hardware companies like Symbolics and Lisp Machines existed just for AI. Software companies sprang up. Japan launched the Fifth Generation Computer Project with massive government backing. The U.S. responded with the Strategic Computing Initiative. Funding tripled between 1984 and 1988.

It was another false summit.

Expert systems worked because they worked in small domains with clear rules. They failed when the world was messy. When assumptions changed. When you needed judgment, not just encoding. By the late 1980s, companies realized they'd spent billions on systems that couldn't do what the sales pitch promised.

In 1987, the specialized AI hardware market collapsed. Desktop computers from Apple and IBM had become faster and cheaper. Why buy a million-dollar Lisp Machine when a \$10,000 PC could do the job? The hype reversed. By 1991, Japan admitted its Fifth Generation goals hadn't been met. By 1993, over 300 companies had shut down or been acquired.

This was the Second AI Winter. It lasted through the 1990s.

\section{Hidden Progress (1990s-2000s)}

Here's what's strange about the second winter: AI didn't stop. It just stopped being called AI.

Researchers started calling their work "machine learning," "computational intelligence," "informatics," "knowledge-based systems"—anything but AI. The name had been poisoned by broken promises.

But underneath, breakthroughs were happening. Probabilistic reasoning. Statistical methods. Neural networks (nobody called them that anymore). Google's search engine used AI techniques. Banks used AI for fraud detection. Logistics companies used AI for routing. Spam filters used AI for classification.

Nick Bostrom's observation captures it: "Once something becomes useful enough and common enough, it's not labeled AI anymore."

By the late 1990s, the field had fragmented into specialized sub-problems. Computer vision. Speech recognition. Natural language processing. Machine learning. Each became a mathematical discipline with rigorous methods. Success came from collaboration with statisticians, mathematicians, electrical engineers, operations researchers. AI became science again instead of prophecy.

Moore's Law was quietly doing the work. Computing power doubled every two years. By 1997, Deep Blue beat Garry Kasparov at chess—not with human-like thinking, but with brute force and smart pruning. A billion calculations a second, evaluating millions of positions. It wasn't elegant. It was effective.

\section{The Machine Learning Turn (2005-2017)}

By 2005, something shifted. Data became abundant. The internet had created digital libraries. Companies had accumulated massive datasets. Fei-Fei Li released ImageNet in 2009—three million labeled images. Word embeddings like word2vec captured meaning in vectors. You could do vector math and get equivalences: King - Man + Woman = Queen.

But the breakthrough came from an old idea made new. Neural networks returned. Deep neural networks. Multiple layers. A technique called backpropagation (developed in the 1970s but never practical) suddenly worked when you had enough data and enough computing power.

In 2012, AlexNet, a deep convolutional neural network, won the ImageNet competition with significantly fewer errors than traditional methods. It was a pivot point. Within years, every other approach to image recognition was abandoned. Deep learning was the new paradigm.

Deep learning worked because it could learn patterns in data without humans hand-coding features. Give it millions of images. It learns. Give it millions of sentences. It learns patterns of language. The constraint wasn't cleverness anymore. It was data and computation.

Computing power was finally catching up. GPUs (graphics processing units) that were designed for video games turned out to be perfect for neural network math. Cloud computing made that power accessible. Amazon, Google, Microsoft all offered AI services in the cloud.

By 2016, investment in AI was surging. Competitions. Conferences. Companies. DeepMind was solving protein folding. OpenAI had just formed. Sentiment shifted. AI wasn't dead. It was coming back.

\section{The Transformer Moment (2017-2022)}

In 2017, researchers at Google published a paper called "Attention is All You Need." The transformer architecture. At the time, it seemed like an incremental improvement. In hindsight, it was the architecture that would change everything.

Transformers could train on massive amounts of text. They could capture long-range dependencies in language. They could be scaled up without losing efficiency. You could train a transformer on billions of words and it would learn statistical patterns about language, knowledge, reasoning, even style.

By 2020, GPT-3 was released. 175 billion parameters trained on hundreds of billions of words from the internet. When you prompted it, it would generate coherent text. It could write code. Tell stories. Explain concepts. It wasn't perfect, but it was remarkable.

Nobody knew it would change the world in two years.

\section{The Interface Shift (2022-2023)}

ChatGPT launched on November 30, 2022. It hit 100 million users in two months. Faster adoption than any application in history.

Here's why it mattered: AI stopped being invisible. It stopped being a feature inside another product. It became the product. An interface you could talk to. Something that felt like intelligence.

It wasn't technically different from GPT-3 (the underlying model was similar). But the interface was. Simple. Accessible. Free. You could write a prompt and get an answer in seconds. You could refine. Iterate. Have a conversation.

Suddenly, non-technical people experienced AI. Lawyers used it. Marketers used it. Product managers used it. Teachers used it. It worked. Not perfectly. But it worked.

The response from big tech was immediate. Google declared "code red." Released Gemini (formerly Bard). Microsoft integrated the technology into Bing Chat. Amazon, Apple, Meta, all scrambled.

By mid-2023, the venture capital world had moved. Generative AI funding skyrocketed. AI companies were raising billions. OpenAI's valuation approached \$86 billion by early 2024.

\section{Why Now. Why This Is Different.}

You could ask: why didn't this happen in 2016? Why not 2018? Why 2022-2023?

Three things converged:

\subsection{Computing power became cheap and accessible.}

The transformer architecture is computationally expensive. Training GPT-3 cost millions of dollars. But once trained, it could run on commodity hardware. Or be rented from the cloud. A startup with \$100K could use world-class AI models. No expensive Lisp Machines. No supercomputers.

Moore's Law had finally done the work. Hardware was fast enough.

\subsection{Data became abundant and labeled.}

ImageNet. Word2vec. Wikipedia. GitHub. The internet itself. Billions of texts, images, code samples. Freely available.

In the 1970s, the bottleneck was data. You couldn't get enough labeled examples. By 2020, the bottleneck had flipped. You had too much data. The challenge was efficiently learning from it.

The transformer architecture turned out to be efficient at scale. Bigger models trained on bigger datasets worked better. The scaling laws held. This was the surprise. Some theorists thought you'd hit diminishing returns. You didn't. You kept getting smarter.

\subsection{The interface made it human.}

GPT-3 was impressive to researchers. ChatGPT was impressive to everyone else.

The difference: a conversation. You could interact with it like a person. Ask follow-ups. Refine your prompt. See it struggle with ambiguity and respond honestly about uncertainty.

That human-like interface meant non-technical people could try it. Use it. Form opinions. Ask for more. The network effects kicked in. Every person who tried ChatGPT told five people. Developers built applications on top of it.

The barrier to adoption disappeared.

\section{The View from 2026}

Here's what we know now, that the 1950s researchers didn't:

Intelligence isn't one thing. General intelligence is possible without understanding. These models don't think like humans. They predict the next token with statistical precision. But that prediction, scaled across billions of parameters and trained on trillions of words, approximates something that looks like reasoning, creativity, judgment.

The walls that stopped AI for 50 years—computing power, data scarcity, architectural limits—turned out to be solvable. Not magically. Through engineering, investment, and time.

But new walls emerged. Models hallucinate. They reflect biases in training data. They can't access real-time information. They struggle with causal reasoning. They don't understand when they should say "I don't know." They consume enormous amounts of energy and resources.

And there's a philosophical question that hasn't been answered: if you can predict the next word so well that it approximates human language, have you created understanding? Or just a very sophisticated mirror?

Product managers don't need to solve that. You need to know: it works well enough for most practical tasks. It's improving. It's accessible. And it changes what's possible in your workflow.

The long road from 1956 to 2023 wasn't a failure. It was prerequisite work. Each failed approach taught something. Each winter created space for new ideas. Each boom and bust clarified what actually mattered.

We're not in a new age because we invented intelligence. We're in a new age because we finally made intelligence accessible.

That's a different thing. And it changes everything that happens next.
